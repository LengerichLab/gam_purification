{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data, originally from:\n",
    "# https://www.kaggle.com/code/danofer/compass-fairml-getting-started/notebook\n",
    "df = pd.read_csv(\"./propublica_data_for_fairml.csv\", header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the data.\n",
    "Y = df['score_factor']\n",
    "X = df.copy()\n",
    "del X['Two_yr_Recidivism'] # Inappropriate to predict with this - (it's an oracle measure of the outcome).\n",
    "del X['score_factor']\n",
    "X = X.astype(float)\n",
    "\n",
    "feature_names = [x for x in X.columns]\n",
    "feature_names\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an EBM to the COMPAS data.\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "ebm1 = ExplainableBoostingClassifier(interactions=0,\n",
    "                                     outer_bags=1,\n",
    "                                     inner_bags=1,\n",
    "                                     early_stopping_rounds=25)\n",
    "ebm2 = ExplainableBoostingClassifier(interactions=50,\n",
    "                                     outer_bags=1,\n",
    "                                     inner_bags=1,\n",
    "                                     early_stopping_rounds=25)\n",
    "ebm1.fit(X_train, Y_train)\n",
    "ebm2.fit(X_train, Y_train)\n",
    "ebm1_global = ebm1.explain_global(name='EBM1')\n",
    "ebm2_global = ebm2.explain_global(name='EBM2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purify the EBM.\n",
    "from gam_purification.models import ebm\n",
    "gam_results = {\n",
    "    \"uniform\": ebm.purify_ebm_uniform(ebm1_global, \"compas\", should_transpose=False),\n",
    "    \"empirical\": ebm.purify_ebm_empirical(ebm1_global, \"compas\", X_train.values, should_transpose=False),\n",
    "    \"laplace\": ebm.purify_ebm_laplace(ebm1_global, \"compas\", X_train.values,\n",
    "                                              laplace=1, should_transpose=False)\n",
    "}\n",
    "ga2m_results = {\n",
    "    \"uniform\": ebm.purify_ebm_uniform(ebm2_global, \"compas\", should_transpose=False),\n",
    "    \"empirical\": ebm.purify_ebm_empirical(ebm2_global, \"compas\", X_train.values, should_transpose=False),\n",
    "    \"laplace\": ebm.purify_ebm_laplace(ebm2_global, \"compas\", X_train.values,\n",
    "                                              laplace=1, should_transpose=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the main effects learned by the EBM under various purification schemes.\n",
    "def plot_mains(feat_name):\n",
    "    idx = feature_names.index(feat_name)\n",
    "    plt.figure()\n",
    "    plt.plot(gam_results['uniform']['mains'][idx], linestyle='--', label=\"GAM\")\n",
    "    plt.plot(gam_results['uniform']['mains_moved'][idx], linestyle='--', label=\"GAM-Uniform\")\n",
    "    plt.plot(gam_results['empirical']['mains_moved'][idx], linestyle='--', label=\"GAM-Laplace\")\n",
    "    plt.plot(gam_results['laplace']['mains_moved'][idx], linestyle='--', label=\"GAM-Laplace\")\n",
    "    \n",
    "    plt.plot(ga2m_results['uniform']['mains'][idx], label=\"GA2M\")\n",
    "    plt.plot(ga2m_results['uniform']['mains_moved'][idx], label=\"GA2M-Uniform\")\n",
    "    plt.plot(ga2m_results['empirical']['mains_moved'][idx], label=\"GA2M-Laplace\")\n",
    "    plt.plot(ga2m_results['laplace']['mains_moved'][idx], label=\"GA2M-Laplace\")\n",
    "    plt.xlabel(feat_name, fontsize=22)\n",
    "    plt.ylabel(\"Addition to Score\", fontsize=22)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    lgd = plt.legend(fontsize=16, bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "for feat_name in feature_names:\n",
    "    plot_mains(feat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit an XGB model to the COMPAS data.\n",
    "import xgboost as xgb\n",
    "from gam_purification.models import xgb as xgb_utils\n",
    "\n",
    "X_train_train, X_val, y_train_train, y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "\n",
    "xgb1 = xgb.XGBClassifier(max_depth=1, learning_rate=0.1, n_estimators=1500)\n",
    "xgb1.fit(X_train_train, y_train_train, eval_set=[(X_val, y_val)],\n",
    "        verbose=False, eval_metric='rmse', early_stopping_rounds=50)\n",
    "\n",
    "xgb2 = xgb.XGBClassifier(max_depth=2, learning_rate=0.1, n_estimators=1500)\n",
    "xgb2.fit(X_train_train, y_train_train, eval_set=[(X_val, y_val)],\n",
    "        verbose=False, eval_metric='rmse', early_stopping_rounds=50)\n",
    "\n",
    "raw_xgb1_dump = xgb1.get_booster().get_dump()\n",
    "raw_xgb2_dump = xgb2.get_booster().get_dump()\n",
    "\n",
    "def get_xgb_pairs(raw_xgb_dump):\n",
    "    xgb_pairs_raw = []\n",
    "    for tree in raw_xgb_dump:\n",
    "        tree1_splits, tree2_splits = xgb_utils.parse_xgb_tree(tree, [])\n",
    "        xgb_pairs_raw.append(tree1_splits)\n",
    "        xgb_pairs_raw.append(tree2_splits)\n",
    "    return xgb_pairs_raw\n",
    "\n",
    "xgb1_pairs_raw = get_xgb_pairs(raw_xgb1_dump)\n",
    "xgb2_pairs_raw = get_xgb_pairs(raw_xgb2_dump)\n",
    "\n",
    "xgb1_mains, xgb1_pairs, n_pairwise, n_marginal = xgb_utils.get_mains_and_pairs(\n",
    "    xgb1_pairs_raw, ebm2_global, ebm2_global.feature_names)\n",
    "xgb2_mains, xgb2_pairs, n_pairwise, n_marginal = xgb_utils.get_mains_and_pairs(\n",
    "    xgb2_pairs_raw, ebm2_global, ebm2_global.feature_names)\n",
    "\n",
    "dataset_name = \"compas\"\n",
    "xgb_results = {\n",
    "    \"uniform\": xgb_utils.purify_xgb_uniform(xgb2_mains, xgb2_pairs, ebm2_global, dataset_name),\n",
    "    \"empirical\": xgb_utils.purify_xgb_empirical(xgb2_mains, xgb2_pairs, ebm2_global, dataset_name,\n",
    "                                                X_train.values),\n",
    "    \"laplace\": xgb_utils.purify_xgb_laplace(xgb2_mains, xgb2_pairs, ebm2_global, dataset_name,\n",
    "                                                   X_train.values, laplace=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the effects learned under different distributions of purification.\n",
    "def diff(results):\n",
    "    return results[1] - results[0]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "n_parts = 12\n",
    "w = 1. / n_parts\n",
    "for idx, feat_name in enumerate(feature_names):\n",
    "    if idx == 0:\n",
    "        plt.bar([(n_parts*idx+1)/n_parts], diff(gam_results['uniform']['mains'][idx]), width=w, label=\"GAM\", color='gray')\n",
    "        plt.bar([(n_parts*idx+2)/n_parts], diff(ga2m_results['uniform']['mains'][idx]), width=w, label=\"GA2M\", color='lightgray')\n",
    "        plt.bar([(n_parts*idx+3)/n_parts], diff(ga2m_results['uniform']['mains_moved'][idx]), width=w, label=\"GA2M-Uniform\", color='blue')\n",
    "        plt.bar([(n_parts*idx+4)/n_parts], diff(ga2m_results['empirical']['mains_moved'][idx]), width=w, label=\"GA2M-Empirical\", color='green')\n",
    "        plt.bar([(n_parts*idx+5)/n_parts], diff(ga2m_results['laplace']['mains_moved'][idx]), width=w, label=\"GA2M-Laplace\", color='red')\n",
    "    else:\n",
    "        plt.bar([(n_parts*idx+1)/n_parts], diff(ga2m_results['uniform']['mains'][idx]), width=w, color='gray')\n",
    "        plt.bar([(n_parts*idx+2)/n_parts], diff(ga2m_results['uniform']['mains'][idx]), width=w, color='lightgray')\n",
    "        plt.bar([(n_parts*idx+3)/n_parts], diff(ga2m_results['uniform']['mains_moved'][idx]), width=w, color='blue')\n",
    "        plt.bar([(n_parts*idx+4)/n_parts], diff(ga2m_results['empirical']['mains_moved'][idx]), width=w, color='green')\n",
    "        plt.bar([(n_parts*idx+5)/n_parts], diff(ga2m_results['laplace']['mains_moved'][idx]), width=w, color='red')\n",
    "    if idx == 0:\n",
    "        plt.bar([(n_parts*idx+6)/n_parts], diff(xgb1_mains.get(feature_names[idx], [0.0, 0.0])), width=w, hatch='/', label=\"XGB\", color='gray')\n",
    "        plt.bar([(n_parts*idx+7)/n_parts], diff(xgb2_mains.get(feature_names[idx], [0.0, 0.0])), width=w, hatch='/', label=\"XGB-2\", color='lightgray')\n",
    "        plt.bar([(n_parts*idx+8)/n_parts], diff(xgb_results['uniform']['mains_moved'].get(idx, [0.0, 0.0])), width=w, hatch='/', label=\"XGB2-Uniform\", color='blue')\n",
    "        plt.bar([(n_parts*idx+9)/n_parts], diff(xgb_results['empirical']['mains_moved'].get(idx, [0.0, 0.0])), width=w, hatch='/', label=\"XGB2-Empirical\", color='green')\n",
    "        plt.bar([(n_parts*idx+10)/n_parts], diff(xgb_results['laplace']['mains_moved'].get(idx, [0.0, 0.0])), width=w, hatch='/', label=\"XGB2-Laplace\", color='red')\n",
    "    else:\n",
    "        plt.bar([(n_parts*idx+6)/n_parts], diff(xgb1_mains.get(feature_names[idx], [0.0, 0.0])), width=w, hatch='/', color='gray')\n",
    "        plt.bar([(n_parts*idx+7)/n_parts], diff(xgb2_mains.get(feature_names[idx], [0.0, 0.0])), width=w, hatch='/', color='lightgray')\n",
    "        plt.bar([(n_parts*idx+8)/n_parts], diff(xgb_results['uniform']['mains_moved'].get(idx, [0.0, 0.0])), width=w, hatch='/', color='blue')\n",
    "        plt.bar([(n_parts*idx+9)/n_parts], diff(xgb_results['empirical']['mains_moved'].get(idx, [0.0, 0.0])), width=w, hatch='/', color='green')\n",
    "        plt.bar([(n_parts*idx+10)/n_parts], diff(xgb_results['laplace']['mains_moved'].get(idx, [0.0, 0.0])), width=w, hatch='/', color='red')\n",
    "    \n",
    "plt.ylim([-3, 4.0])\n",
    "lgd = plt.legend(bbox_to_anchor=(1., 1.0), fontsize=18)\n",
    "plt.xticks([0.4 + x for x in range(len(feature_names))], feature_names, rotation=70, fontsize=20)\n",
    "plt.ylabel(\"Addition to Score\", fontsize=26)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
